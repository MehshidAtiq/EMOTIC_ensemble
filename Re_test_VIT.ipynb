{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3Vn8WPUTDFx"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import Libraries and Dependencies\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
        "from torch import nn\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_jPTMH7V7BA"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Dataset Class Definition\n",
        "class EMOTICDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations, img_dir, feature_extractor, num_categories=26):\n",
        "        self.annotations = annotations\n",
        "        self.img_dir = img_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.num_categories = num_categories\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.annotations[idx]\n",
        "        img_path = os.path.join(self.img_dir, entry['filename'])\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"File not found: {img_path}\")\n",
        "\n",
        "        # Load the image and ensure it's RGB\n",
        "        image = np.load(img_path)\n",
        "        if len(image.shape) == 2:  # Grayscale image\n",
        "            image = np.stack([image] * 3, axis=-1)\n",
        "\n",
        "        # Preprocess the image\n",
        "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\", antialias=True)\n",
        "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
        "\n",
        "        # Multi-hot encoding for labels\n",
        "        categories = torch.zeros(self.num_categories, dtype=torch.float32)\n",
        "        for category in entry['categories']:\n",
        "            if category < self.num_categories:\n",
        "                categories[category] = 1.0\n",
        "\n",
        "        inputs[\"labels\"] = categories\n",
        "        return inputs\n",
        "\n",
        "def parse_annotations(csv_path):\n",
        "    \"\"\"Parse annotations from CSV file\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    category_columns = df.columns[8:34]\n",
        "\n",
        "    # Calculate class counts\n",
        "    class_counts = df[category_columns].sum().to_numpy(dtype=np.float32)\n",
        "\n",
        "    # Parse annotations\n",
        "    annotations = []\n",
        "    for _, row in df.iterrows():\n",
        "        categories = [int(idx) for idx, val in enumerate(row[category_columns]) if val == 1]\n",
        "        annotation = {\"filename\": row[\"Crop_name\"], \"categories\": categories}\n",
        "        annotations.append(annotation)\n",
        "\n",
        "    return annotations, class_counts\n",
        "\n",
        "print(\"Dataset classes defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1RbQF2RWCqa"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Configuration and Paths Setup\n",
        "# UPDATE THESE PATHS TO MATCH YOUR SETUP\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = \"best_vit_emotic.pth\"\n",
        "val_annotations_path = \"/content/emotic_data/annots_arrs/annot_arrs_val.csv\"\n",
        "test_annotations_path = \"/content/emotic_data/annots_arrs/annot_arrs_val.csv\"  # Using val as test for now\n",
        "img_dir = \"/content/emotic_data/img_arrs/\"\n",
        "\n",
        "# Parameters\n",
        "batch_size = 32\n",
        "num_classes = 26\n",
        "emotion_labels = [\n",
        "    'Peace', 'Affection', 'Esteem', 'Anticipation', 'Engagement',\n",
        "    'Confidence', 'Happiness', 'Pleasure', 'Excitement', 'Surprise',\n",
        "    'Sympathy', 'Doubt/Confusion', 'Disconnection', 'Fatigue',\n",
        "    'Embarrassment', 'Yearning', 'Disapproval', 'Aversion',\n",
        "    'Annoyance', 'Anger', 'Sensitivity', 'Sadness',\n",
        "    'Disquietment', 'Fear', 'Pain', 'Suffering'\n",
        "]\n",
        "\n",
        "print(f\"Configuration set:\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model path: {model_path}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Batch size: {batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrP75Ox_WG-s"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Model Loading\n",
        "def load_model(model_path, device, num_classes=26):\n",
        "    \"\"\"Load the trained model\"\"\"\n",
        "    model = AutoModelForImageClassification.from_pretrained(\n",
        "        \"google/vit-base-patch16-224\",\n",
        "        ignore_mismatched_sizes=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Recreate the classifier head\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(model.config.hidden_size, num_classes)\n",
        "    ).to(device)\n",
        "\n",
        "    # Load trained weights\n",
        "    if os.path.exists(model_path):\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        print(f\"✓ Model loaded successfully from {model_path}\")\n",
        "    else:\n",
        "        print(f\"⚠️  Warning: Model file {model_path} not found. Using randomly initialized model.\")\n",
        "\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Load the model\n",
        "model = load_model(model_path, device, num_classes)\n",
        "feature_extractor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n",
        "\n",
        "print(\"Model loaded and ready for testing!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASXRO2NVXf39"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPbpY5LzWKPY"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Dataset Loading and Basic Info\n",
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "val_annotations, val_class_counts = parse_annotations(val_annotations_path)\n",
        "test_annotations, test_class_counts = parse_annotations(test_annotations_path)\n",
        "\n",
        "val_dataset = EMOTICDataset(val_annotations, img_dir, feature_extractor, num_classes)\n",
        "test_dataset = EMOTICDataset(test_annotations, img_dir, feature_extractor, num_classes)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✓ Validation set: {len(val_dataset)} samples\")\n",
        "print(f\"✓ Test set: {len(test_dataset)} samples\")\n",
        "\n",
        "# Display class distribution\n",
        "class_dist_df = pd.DataFrame({\n",
        "    'Emotion': emotion_labels,\n",
        "    'Val_Count': val_class_counts,\n",
        "    'Test_Count': test_class_counts,\n",
        "    'Val_Percentage': val_class_counts / len(val_dataset) * 100,\n",
        "    'Test_Percentage': test_class_counts / len(test_dataset) * 100\n",
        "})\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "print(class_dist_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "015lPVsNWS1X"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Basic Model Prediction Test\n",
        "def test_single_batch(model, data_loader, device):\n",
        "    \"\"\"Test model on a single batch to verify it's working\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get one batch\n",
        "    batch = next(iter(data_loader))\n",
        "    images = batch[\"pixel_values\"].to(device)\n",
        "    labels = batch[\"labels\"]\n",
        "\n",
        "    print(f\"Batch shape: {images.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "    print(f\"Logits shape: {logits.shape}\")\n",
        "    print(f\"Logits range: [{logits.min().item():.4f}, {logits.max().item():.4f}]\")\n",
        "    print(f\"Probabilities range: [{probs.min().item():.4f}, {probs.max().item():.4f}]\")\n",
        "    print(f\"Mean probability: {probs.mean().item():.4f}\")\n",
        "\n",
        "    # Show predictions for first sample\n",
        "    first_sample_probs = probs[0].cpu().numpy()\n",
        "    first_sample_labels = labels[0].numpy()\n",
        "\n",
        "    print(f\"\\nFirst sample predictions:\")\n",
        "    print(f\"True labels: {np.where(first_sample_labels == 1)[0]}\")\n",
        "    print(f\"Top 5 predicted emotions (with probabilities):\")\n",
        "    top_indices = np.argsort(first_sample_probs)[-5:][::-1]\n",
        "    for idx in top_indices:\n",
        "        print(f\"  {emotion_labels[idx]}: {first_sample_probs[idx]:.4f}\")\n",
        "\n",
        "    return logits.cpu().numpy(), probs.cpu().numpy(), labels.numpy()\n",
        "\n",
        "print(\"Testing model on a single batch...\")\n",
        "test_logits, test_probs, test_labels = test_single_batch(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffpIgehDWXSa"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Get All Predictions and Targets\n",
        "def get_predictions_and_targets(model, data_loader, device):\n",
        "    \"\"\"Get model predictions and ground truth targets for entire dataset\"\"\"\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "    all_logits = []\n",
        "\n",
        "    model.eval()\n",
        "    print(\"Getting predictions for entire dataset...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Processing batch {i+1}/{len(data_loader)}\")\n",
        "\n",
        "            images = batch[\"pixel_values\"].to(device)\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "            all_targets.extend(labels.numpy())\n",
        "            all_probs.extend(probs)\n",
        "            all_logits.extend(logits.cpu().numpy())\n",
        "\n",
        "    all_targets = np.array(all_targets)\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_logits = np.array(all_logits)\n",
        "\n",
        "    print(f\"✓ Collected predictions from {len(all_targets)} samples\")\n",
        "    print(f\"Targets shape: {all_targets.shape}\")\n",
        "    print(f\"Probabilities shape: {all_probs.shape}\")\n",
        "\n",
        "    return all_targets, all_probs, all_logits\n",
        "\n",
        "# Get predictions for validation set (for threshold calculation)\n",
        "val_targets, val_probs, val_logits = get_predictions_and_targets(model, val_loader, device)\n",
        "\n",
        "# Get predictions for test set (for final evaluation)\n",
        "test_targets, test_probs, test_logits = get_predictions_and_targets(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8PagmRQWbLI"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Threshold Optimization\n",
        "def calculate_optimal_thresholds(targets, probs, emotion_labels, threshold_range=(0.001, 0.5, 0.01)):\n",
        "    \"\"\"Calculate optimal thresholds for each class based on F1 score\"\"\"\n",
        "    print(\"Calculating optimal thresholds for each emotion class...\")\n",
        "\n",
        "    optimal_thresholds = []\n",
        "    threshold_details = []\n",
        "\n",
        "    for class_idx in range(len(emotion_labels)):\n",
        "        if class_idx % 5 == 0:\n",
        "            print(f\"Processing classes {class_idx}-{min(class_idx+4, len(emotion_labels)-1)}...\")\n",
        "\n",
        "        best_threshold = 0.1\n",
        "        best_f1 = 0.0\n",
        "        class_details = []\n",
        "\n",
        "        # Skip classes with no positive samples\n",
        "        if targets[:, class_idx].sum() == 0:\n",
        "            print(f\"⚠️  {emotion_labels[class_idx]}: No positive samples, using default threshold\")\n",
        "            optimal_thresholds.append(0.1)\n",
        "            threshold_details.append({\n",
        "                'class': emotion_labels[class_idx],\n",
        "                'best_threshold': 0.1,\n",
        "                'best_f1': 0.0,\n",
        "                'num_positives': 0\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        for threshold in np.arange(*threshold_range):\n",
        "            preds = (probs[:, class_idx] > threshold).astype(int)\n",
        "\n",
        "            # Skip if no predictions made\n",
        "            if preds.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                targets[:, class_idx], preds, average='binary', zero_division=0\n",
        "            )\n",
        "\n",
        "            class_details.append({\n",
        "                'threshold': threshold,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            })\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        optimal_thresholds.append(best_threshold)\n",
        "        threshold_details.append({\n",
        "            'class': emotion_labels[class_idx],\n",
        "            'best_threshold': best_threshold,\n",
        "            'best_f1': best_f1,\n",
        "            'num_positives': int(targets[:, class_idx].sum()),\n",
        "            'details': class_details\n",
        "        })\n",
        "\n",
        "    return np.array(optimal_thresholds), threshold_details\n",
        "\n",
        "# Calculate optimal thresholds using validation set\n",
        "optimal_thresholds, threshold_details = calculate_optimal_thresholds(\n",
        "    val_targets, val_probs, emotion_labels\n",
        ")\n",
        "\n",
        "# Display threshold results\n",
        "print(\"\\nOptimal Thresholds Summary:\")\n",
        "print(\"-\" * 60)\n",
        "for i, detail in enumerate(threshold_details):\n",
        "    print(f\"{detail['class']:<20}: threshold={detail['best_threshold']:.4f}, \"\n",
        "          f\"F1={detail['best_f1']:.4f}, positives={detail['num_positives']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN8laHkaWeZe"
      },
      "outputs": [],
      "source": [
        "# Cell 9: mAP Calculation\n",
        "def calculate_map(targets, probs, emotion_labels):\n",
        "    \"\"\"Calculate mean Average Precision (mAP)\"\"\"\n",
        "    print(\"Calculating mean Average Precision (mAP)...\")\n",
        "\n",
        "    ap_scores = []\n",
        "    valid_classes = 0\n",
        "\n",
        "    for class_idx in range(len(emotion_labels)):\n",
        "        # Skip classes with no positive samples\n",
        "        if targets[:, class_idx].sum() == 0:\n",
        "            print(f\"⚠️  {emotion_labels[class_idx]}: No positive samples, AP = 0\")\n",
        "            ap_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        ap = average_precision_score(targets[:, class_idx], probs[:, class_idx])\n",
        "        ap_scores.append(ap)\n",
        "        valid_classes += 1\n",
        "\n",
        "        if class_idx < 5:  # Show first 5 for debugging\n",
        "            print(f\"✓ {emotion_labels[class_idx]}: AP = {ap:.4f}\")\n",
        "\n",
        "    map_score = np.mean(ap_scores)\n",
        "    print(f\"\\n✓ Mean Average Precision (mAP): {map_score:.4f}\")\n",
        "    print(f\"✓ Valid classes for mAP calculation: {valid_classes}/{len(emotion_labels)}\")\n",
        "\n",
        "    return map_score, ap_scores\n",
        "\n",
        "# Calculate mAP on test set\n",
        "test_map_score, test_ap_scores = calculate_map(test_targets, test_probs, emotion_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhnOnxH-WiKx"
      },
      "outputs": [],
      "source": [
        "# Cell 10: F1 Score Calculation\n",
        "def calculate_f1_scores(targets, probs, thresholds, emotion_labels):\n",
        "    \"\"\"Calculate F1 scores using optimal thresholds\"\"\"\n",
        "    print(\"Calculating F1 scores with optimal thresholds...\")\n",
        "\n",
        "    # Apply thresholds\n",
        "    predictions = np.zeros_like(probs)\n",
        "    for class_idx in range(len(emotion_labels)):\n",
        "        predictions[:, class_idx] = (probs[:, class_idx] > thresholds[class_idx]).astype(int)\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    f1_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "\n",
        "    for class_idx in range(len(emotion_labels)):\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            targets[:, class_idx], predictions[:, class_idx],\n",
        "            average='binary', zero_division=0\n",
        "        )\n",
        "        f1_scores.append(f1)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "\n",
        "    # Calculate macro averages\n",
        "    macro_f1 = np.mean(f1_scores)\n",
        "    macro_precision = np.mean(precision_scores)\n",
        "    macro_recall = np.mean(recall_scores)\n",
        "\n",
        "    # Calculate exact match accuracy\n",
        "    exact_match_accuracy = np.mean(np.all(targets == predictions, axis=1))\n",
        "\n",
        "    print(f\"✓ Macro F1 Score: {macro_f1:.4f}\")\n",
        "    print(f\"✓ Macro Precision: {macro_precision:.4f}\")\n",
        "    print(f\"✓ Macro Recall: {macro_recall:.4f}\")\n",
        "    print(f\"✓ Exact Match Accuracy: {exact_match_accuracy:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'macro_f1': macro_f1,\n",
        "        'macro_precision': macro_precision,\n",
        "        'macro_recall': macro_recall,\n",
        "        'exact_match_accuracy': exact_match_accuracy,\n",
        "        'per_class_f1': f1_scores,\n",
        "        'per_class_precision': precision_scores,\n",
        "        'per_class_recall': recall_scores,\n",
        "        'predictions': predictions\n",
        "    }\n",
        "\n",
        "# Calculate F1 scores on test set\n",
        "f1_results = calculate_f1_scores(test_targets, test_probs, optimal_thresholds, emotion_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDP67Zv9WlTB"
      },
      "outputs": [],
      "source": [
        "# Cell 11: Results Summary and Per-Class Analysis\n",
        "print(\"=\"*80)\n",
        "print(\"FINAL MODEL EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Mean Average Precision (mAP): {test_map_score:.4f}\")\n",
        "print(f\"Macro F1 Score: {f1_results['macro_f1']:.4f}\")\n",
        "print(f\"Macro Precision: {f1_results['macro_precision']:.4f}\")\n",
        "print(f\"Macro Recall: {f1_results['macro_recall']:.4f}\")\n",
        "print(f\"Exact Match Accuracy: {f1_results['exact_match_accuracy']:.4f}\")\n",
        "\n",
        "# Create detailed results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Emotion': emotion_labels,\n",
        "    'Average_Precision': test_ap_scores,\n",
        "    'F1_Score': f1_results['per_class_f1'],\n",
        "    'Precision': f1_results['per_class_precision'],\n",
        "    'Recall': f1_results['per_class_recall'],\n",
        "    'Optimal_Threshold': optimal_thresholds,\n",
        "    'Num_Positives': [int(test_targets[:, i].sum()) for i in range(num_classes)]\n",
        "})\n",
        "\n",
        "print(f\"\\nDetailed Per-Class Results:\")\n",
        "print(\"-\" * 100)\n",
        "print(results_df.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('detailed_model_results.csv', index=False)\n",
        "print(f\"\\n✓ Detailed results saved to 'detailed_model_results.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P87_1gO4Wogn"
      },
      "outputs": [],
      "source": [
        "# Cell 12: Visualizations\n",
        "def create_evaluation_plots(results_df, test_map_score, macro_f1):\n",
        "    \"\"\"Create comprehensive evaluation plots\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Plot 1: Per-class AP scores\n",
        "    axes[0, 0].barh(range(len(emotion_labels)), results_df['Average_Precision'], color='skyblue')\n",
        "    axes[0, 0].set_yticks(range(len(emotion_labels)))\n",
        "    axes[0, 0].set_yticklabels(emotion_labels, fontsize=8)\n",
        "    axes[0, 0].set_xlabel('Average Precision')\n",
        "    axes[0, 0].set_title(f'Per-Class Average Precision (mAP: {test_map_score:.4f})')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].axvline(x=test_map_score, color='red', linestyle='--', alpha=0.7, label=f'mAP: {test_map_score:.4f}')\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    # Plot 2: Per-class F1 scores\n",
        "    axes[0, 1].barh(range(len(emotion_labels)), results_df['F1_Score'], color='lightgreen')\n",
        "    axes[0, 1].set_yticks(range(len(emotion_labels)))\n",
        "    axes[0, 1].set_yticklabels(emotion_labels, fontsize=8)\n",
        "    axes[0, 1].set_xlabel('F1 Score')\n",
        "    axes[0, 1].set_title(f'Per-Class F1 Scores (Macro F1: {macro_f1:.4f})')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].axvline(x=macro_f1, color='red', linestyle='--', alpha=0.7, label=f'Macro F1: {macro_f1:.4f}')\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # Plot 3: Optimal thresholds\n",
        "    axes[1, 0].barh(range(len(emotion_labels)), results_df['Optimal_Threshold'], color='orange')\n",
        "    axes[1, 0].set_yticks(range(len(emotion_labels)))\n",
        "    axes[1, 0].set_yticklabels(emotion_labels, fontsize=8)\n",
        "    axes[1, 0].set_xlabel('Optimal Threshold')\n",
        "    axes[1, 0].set_title('Optimal Thresholds per Class')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Precision vs Recall scatter\n",
        "    scatter = axes[1, 1].scatter(results_df['Recall'], results_df['Precision'],\n",
        "                                c=results_df['F1_Score'], cmap='viridis', s=60, alpha=0.7)\n",
        "    axes[1, 1].set_xlabel('Recall')\n",
        "    axes[1, 1].set_ylabel('Precision')\n",
        "    axes[1, 1].set_title('Precision vs Recall per Class (colored by F1)')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    plt.colorbar(scatter, ax=axes[1, 1], label='F1 Score')\n",
        "\n",
        "    # Add diagonal line for reference\n",
        "    max_val = max(results_df['Recall'].max(), results_df['Precision'].max())\n",
        "    axes[1, 1].plot([0, max_val], [0, max_val], 'k--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ Evaluation plots saved to 'model_evaluation_results.png'\")\n",
        "    plt.show()\n",
        "\n",
        "# Create and display plots\n",
        "create_evaluation_plots(results_df, test_map_score, f1_results['macro_f1'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lrc4pOiwWrzO"
      },
      "outputs": [],
      "source": [
        "# Cell 13: Additional Analysis - Top and Bottom Performing Classes\n",
        "print(\"=\"*60)\n",
        "print(\"TOP AND BOTTOM PERFORMING CLASSES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sort by F1 score\n",
        "sorted_results = results_df.sort_values('F1_Score', ascending=False)\n",
        "\n",
        "print(\"Top 5 performing classes (by F1 score):\")\n",
        "print(\"-\" * 40)\n",
        "top_5 = sorted_results.head(5)\n",
        "for _, row in top_5.iterrows():\n",
        "    print(f\"{row['Emotion']:<20}: F1={row['F1_Score']:.4f}, AP={row['Average_Precision']:.4f}\")\n",
        "\n",
        "print(\"\\nBottom 5 performing classes (by F1 score):\")\n",
        "print(\"-\" * 40)\n",
        "bottom_5 = sorted_results.tail(5)\n",
        "for _, row in bottom_5.iterrows():\n",
        "    print(f\"{row['Emotion']:<20}: F1={row['F1_Score']:.4f}, AP={row['Average_Precision']:.4f}\")\n",
        "\n",
        "# Analyze class imbalance impact\n",
        "print(f\"\\nClass Imbalance Analysis:\")\n",
        "print(\"-\" * 40)\n",
        "results_df['Imbalance_Ratio'] = results_df['Num_Positives'] / len(test_targets)\n",
        "correlation_f1_balance = results_df[['F1_Score', 'Imbalance_Ratio']].corr().iloc[0, 1]\n",
        "correlation_ap_balance = results_df[['Average_Precision', 'Imbalance_Ratio']].corr().iloc[0, 1]\n",
        "\n",
        "print(f\"Correlation between F1 and class balance: {correlation_f1_balance:.4f}\")\n",
        "print(f\"Correlation between AP and class balance: {correlation_ap_balance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlUTwkdWWvkD"
      },
      "outputs": [],
      "source": [
        "# Cell 14: Model Comparison with Different Thresholds\n",
        "def compare_threshold_strategies(targets, probs, optimal_thresholds, emotion_labels):\n",
        "    \"\"\"Compare different threshold strategies\"\"\"\n",
        "    print(\"Comparing different threshold strategies...\")\n",
        "\n",
        "    strategies = {\n",
        "        'Fixed_0.5': np.full(len(emotion_labels), 0.5),\n",
        "        'Fixed_0.3': np.full(len(emotion_labels), 0.3),\n",
        "        'Fixed_0.1': np.full(len(emotion_labels), 0.1),\n",
        "        'Optimal': optimal_thresholds\n",
        "    }\n",
        "\n",
        "    comparison_results = []\n",
        "\n",
        "    for strategy_name, thresholds in strategies.items():\n",
        "        results = calculate_f1_scores(targets, probs, thresholds, emotion_labels)\n",
        "        comparison_results.append({\n",
        "            'Strategy': strategy_name,\n",
        "            'Macro_F1': results['macro_f1'],\n",
        "            'Macro_Precision': results['macro_precision'],\n",
        "            'Macro_Recall': results['macro_recall'],\n",
        "            'Exact_Match_Acc': results['exact_match_accuracy']\n",
        "        })\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_results)\n",
        "    print(\"\\nThreshold Strategy Comparison:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Compare threshold strategies\n",
        "threshold_comparison = compare_threshold_strategies(\n",
        "    test_targets, test_probs, optimal_thresholds, emotion_labels\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"Files saved:\")\n",
        "print(\"- detailed_model_results.csv: Per-class detailed results\")\n",
        "print(\"- model_evaluation_results.png: Visualization plots\")\n",
        "print(\"\\nAll evaluation metrics have been calculated and saved.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
