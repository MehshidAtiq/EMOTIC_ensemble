{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies with specific versions for compatibility\n",
    "!pip uninstall torch torchvision transformers numpy -y  # Clean uninstall first\n",
    "!pip install numpy==1.26.4  # Install NumPy first to avoid conflicts\n",
    "!pip install torch==2.1.2 torchvision==0.16.2\n",
    "!pip install transformers==4.36.2 datasets scikit-learn pandas kaggle tqdm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff7f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/bn/_0y2n49d3_5cqf7bq3t0pmp40000gn/T/ipykernel_72609/1159291889.py\", line 10, in <module>\n",
      "    from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py\", line 2292, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py\", line 2320, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/image_processing_auto.py\", line 27, in <module>\n",
      "    from ...image_processing_utils import ImageProcessingMixin\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/transformers/image_processing_utils.py\", line 22, in <module>\n",
      "    from .image_transforms import center_crop, normalize, rescale\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/transformers/image_transforms.py\", line 22, in <module>\n",
      "    from .image_utils import (\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/transformers/image_utils.py\", line 59, in <module>\n",
      "    from torchvision.transforms import InterpolationMode\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/torchvision/__init__.py\", line 6, in <module>\n",
      "    from torchvision import datasets, io, models, ops, transforms, utils\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
      "    from . import detection, optical_flow, quantization, segmentation, video\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
      "    from .faster_rcnn import *\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
      "    from .anchor_utils import AnchorGenerator\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
      "    class AnchorGenerator(nn.Module):\n",
      "  File \"/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
      "    device: torch.device = torch.device(\"cpu\"),\n",
      "/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/mehshidatiq/Library/Python/3.9/lib/python/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import ViTForImageClassification\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f023f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 464\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd30f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device - MacBook compatible\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")  # Apple Silicon GPU\n",
    "    else:\n",
    "        return torch.device(\"cpu\")  # Fallback to CPU\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe753b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(csv_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Select only numeric category columns (9:35 worked assumed)\n",
    "    category_columns = df.columns[8:34]\n",
    "\n",
    "    # Debugging: Check if the selected columns are numeric\n",
    "    print(\"Selected category columns:\", category_columns)\n",
    "    print(\"Column types:\", df[category_columns].dtypes)\n",
    "\n",
    "    # Ensure all data in category columns is numeric\n",
    "    for col in category_columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            raise ValueError(f\"Column {col} contains non-numeric data.\")\n",
    "\n",
    "    # Calculate class counts\n",
    "    class_counts = df[category_columns].sum().to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Parse annotations\n",
    "    annotations = []\n",
    "    for _, row in df.iterrows():\n",
    "        categories = [int(idx) for idx, val in enumerate(row[category_columns]) if val == 1]\n",
    "        annotation = {\"filename\": row[\"Crop_name\"], \"categories\": categories}\n",
    "        annotations.append(annotation)\n",
    "\n",
    "    return annotations, class_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198665ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def calculate_map_and_f1(all_targets, all_predictions, all_probabilities):\n",
    "    \"\"\"\n",
    "    Calculate mAP and F1 scores for multi-label classification.\n",
    "    \"\"\"\n",
    "    # Calculate mAP\n",
    "    map_scores = []\n",
    "    for i in range(all_targets.shape[1]):\n",
    "        if np.sum(all_targets[:, i]) > 0:  # Only calculate if positive samples exist\n",
    "            ap = average_precision_score(all_targets[:, i], all_probabilities[:, i])\n",
    "            map_scores.append(ap)\n",
    "    \n",
    "    mean_ap = np.mean(map_scores) if map_scores else 0.0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_targets, all_predictions, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    \n",
    "    return mean_ap, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff03e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMOTICDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations, img_dir, feature_extractor, num_categories=26):\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.num_categories = num_categories\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.annotations[idx]\n",
    "        img_path = os.path.join(self.img_dir, entry['filename'])\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"File not found: {img_path}\")\n",
    "\n",
    "        # Load the image and ensure it's RGB\n",
    "        image = np.load(img_path)\n",
    "        if len(image.shape) == 2:  # Grayscale image\n",
    "            image = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "        # Preprocess the image\n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {key: val.squeeze(0).to(device) for key, val in inputs.items()}\n",
    "\n",
    "        # Multi-hot encoding for labels\n",
    "        categories = torch.zeros(self.num_categories, dtype=torch.float32).to(device)\n",
    "        for category in entry['categories']:\n",
    "            if category < self.num_categories:\n",
    "                categories[category] = 1.0\n",
    "\n",
    "        inputs[\"labels\"] = categories\n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6eddd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomViTForImageClassification(ViTForImageClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        interpolate_pos_encoding=None,\n",
    "        return_dict=None\n",
    "    ):\n",
    "        # Outputs from the ViT model\n",
    "        outputs = self.vit(\n",
    "            pixel_values,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            interpolate_pos_encoding=interpolate_pos_encoding,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        # Ensure sequence_output is on the same device as the classifier\n",
    "        sequence_output = sequence_output.to(next(self.classifier.parameters()).device)\n",
    "        logits = self.classifier(sequence_output[:, 0, :])\n",
    "        return type('ModelOutput', (), {'logits': logits})()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ded4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        ce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "\n",
    "        # Modulation\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        loss = ce_loss * ((1 - p_t) ** self.gamma)\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            loss = alpha_t * loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29181e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dynamic_thresholds(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Calculate optimal thresholds for each label based on F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(images).logits\n",
    "            probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(probabilities)\n",
    "\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    all_outputs = np.vstack(all_outputs)\n",
    "\n",
    "    # Adjust thresholds to consider the low logit values\n",
    "    thresholds = []\n",
    "    for i in range(all_targets.shape[1]):\n",
    "        best_threshold = 0.1\n",
    "        best_f1 = 0\n",
    "        print(f\"Processing Class {i}\")\n",
    "        for threshold in np.arange(0.001, 0.2, 0.01):\n",
    "            preds = (all_outputs[:, i] > threshold).astype(int)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                all_targets[:, i], preds, average=\"binary\", zero_division=0\n",
    "            )\n",
    "            print(f\"Threshold: {threshold:.2f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        thresholds.append(best_threshold)\n",
    "        print(f\"Best Threshold for Class {i}: {best_threshold}, Best F1: {best_f1:.4f}\")\n",
    "    \n",
    "    print(f\"Dynamic Thresholds: {thresholds}\")\n",
    "    return np.array(thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5963fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device, thresholds=None):\n",
    "    \"\"\"\n",
    "    Validate the model and compute mAP and F1 metrics only.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(images).logits\n",
    "            probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            # Use dynamic thresholds if provided\n",
    "            if thresholds is not None:\n",
    "                predictions = (probabilities > thresholds).astype(int)\n",
    "            else:\n",
    "                predictions = (probabilities > 0.1).astype(int)\n",
    "\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions)\n",
    "            all_probabilities.extend(probabilities)\n",
    "\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_probabilities = np.vstack(all_probabilities)\n",
    "\n",
    "    # Calculate mAP and F1 scores\n",
    "    mean_ap, f1, precision, recall = calculate_map_and_f1(all_targets, all_predictions, all_probabilities)\n",
    "\n",
    "    print(f\"Validation Metrics - mAP: {mean_ap:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    return mean_ap, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0135b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_dynamic_weights(\n",
    "    model, train_loader, val_loader, optimizer, num_epochs, device, model_path, class_counts\n",
    "):\n",
    "    class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32, device=device)\n",
    "    criterion_bce = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "    criterion_focal = FocalLoss(gamma=2, alpha=class_weights)\n",
    "\n",
    "    # Remove CUDA-specific scaler for Mac compatibility\n",
    "    scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    auxiliary_weight = 0.5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for batch in train_loader_iter:\n",
    "            images = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            model.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Remove autocast for Mac compatibility\n",
    "            logits = model(images).logits\n",
    "\n",
    "            loss_bce = criterion_bce(logits, labels)\n",
    "            loss_focal = criterion_focal(logits, labels)\n",
    "            total_loss_batch = loss_bce + auxiliary_weight * loss_focal\n",
    "\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loader_iter.set_postfix(loss=total_loss_batch.item())\n",
    "            total_loss += total_loss_batch.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        mean_ap, f1, precision, recall = validate_model(model, val_loader, device)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, F1: {f1:.4f}, mAP: {mean_ap:.4f}\")\n",
    "        \n",
    "        print(f\"Auxiliary Weight Before Adjustment: {auxiliary_weight}\")\n",
    "\n",
    "        if f1 < 0.7:\n",
    "            auxiliary_weight = 0.7\n",
    "        elif f1 > 0.9:\n",
    "            auxiliary_weight = 0.3\n",
    "        else:\n",
    "            auxiliary_weight = 0.5\n",
    "\n",
    "        print(f\"Auxiliary Weight After Adjustment: {auxiliary_weight}\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Model saved with F1 score: {best_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fd3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9291186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.image_processing_vit_fast._LazyModule.__getattr__.<locals>.Placeholder'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotations...\n",
      "Selected category columns: Index(['Peace', 'Affection', 'Esteem', 'Anticipation', 'Engagement',\n",
      "       'Confidence', 'Happiness', 'Pleasure', 'Excitement', 'Surprise',\n",
      "       'Sympathy', 'Doubt/Confusion', 'Disconnection', 'Fatigue',\n",
      "       'Embarrassment', 'Yearning', 'Disapproval', 'Aversion', 'Annoyance',\n",
      "       'Anger', 'Sensitivity', 'Sadness', 'Disquietment', 'Fear', 'Pain',\n",
      "       'Suffering'],\n",
      "      dtype='object')\n",
      "Column types: Peace              float64\n",
      "Affection          float64\n",
      "Esteem             float64\n",
      "Anticipation       float64\n",
      "Engagement         float64\n",
      "Confidence         float64\n",
      "Happiness          float64\n",
      "Pleasure           float64\n",
      "Excitement         float64\n",
      "Surprise           float64\n",
      "Sympathy           float64\n",
      "Doubt/Confusion    float64\n",
      "Disconnection      float64\n",
      "Fatigue            float64\n",
      "Embarrassment      float64\n",
      "Yearning           float64\n",
      "Disapproval        float64\n",
      "Aversion           float64\n",
      "Annoyance          float64\n",
      "Anger              float64\n",
      "Sensitivity        float64\n",
      "Sadness            float64\n",
      "Disquietment       float64\n",
      "Fear               float64\n",
      "Pain               float64\n",
      "Suffering          float64\n",
      "dtype: object\n",
      "Selected category columns: Index(['Peace', 'Affection', 'Esteem', 'Anticipation', 'Engagement',\n",
      "       'Confidence', 'Happiness', 'Pleasure', 'Excitement', 'Surprise',\n",
      "       'Sympathy', 'Doubt/Confusion', 'Disconnection', 'Fatigue',\n",
      "       'Embarrassment', 'Yearning', 'Disapproval', 'Aversion', 'Annoyance',\n",
      "       'Anger', 'Sensitivity', 'Sadness', 'Disquietment', 'Fear', 'Pain',\n",
      "       'Suffering'],\n",
      "      dtype='object')\n",
      "Column types: Peace              float64\n",
      "Affection          float64\n",
      "Esteem             float64\n",
      "Anticipation       float64\n",
      "Engagement         float64\n",
      "Confidence         float64\n",
      "Happiness          float64\n",
      "Pleasure           float64\n",
      "Excitement         float64\n",
      "Surprise           float64\n",
      "Sympathy           float64\n",
      "Doubt/Confusion    float64\n",
      "Disconnection      float64\n",
      "Fatigue            float64\n",
      "Embarrassment      float64\n",
      "Yearning           float64\n",
      "Disapproval        float64\n",
      "Aversion           float64\n",
      "Annoyance          float64\n",
      "Anger              float64\n",
      "Sensitivity        float64\n",
      "Sadness            float64\n",
      "Disquietment       float64\n",
      "Fear               float64\n",
      "Pain               float64\n",
      "Suffering          float64\n",
      "dtype: object\n",
      "Creating datasets...\n",
      "Loading model...\n",
      "Direct import failed: \n",
      "ViTForImageClassification requires the PyTorch library but it was not found in your environment. Check out the instructions on the\n",
      "installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
      "Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Trying alternative method...\n",
      "Alternative import also failed: \n",
      "ViTForImageClassification requires the PyTorch library but it was not found in your environment. Check out the instructions on the\n",
      "installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
      "Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Falling back to original AutoModel (may fail)...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForImageClassification requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViTForImageClassification\n\u001b[0;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mViTForImageClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39mnum_classes,\n\u001b[1;32m     38\u001b[0m     ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully using direct ViT import!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:2132\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   2131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 2132\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:2118\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 2118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nViTForImageClassification requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViTForImageClassification\n\u001b[0;32m---> 50\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mViTForImageClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39mnum_classes,\n\u001b[1;32m     53\u001b[0m     ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     54\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully using models module!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:2132\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   2131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 2132\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:2118\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 2118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nViTForImageClassification requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to original AutoModel (may fail)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# Fallback to original method\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForImageClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     64\u001b[0m             num_labels\u001b[38;5;241m=\u001b[39mnum_classes,\n\u001b[1;32m     65\u001b[0m             ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Replace classifier\u001b[39;00m\n\u001b[1;32m     69\u001b[0m model\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     70\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.3\u001b[39m),\n\u001b[1;32m     71\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, num_classes)\n\u001b[1;32m     72\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:2132\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   2130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 2132\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:2118\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   2115\u001b[0m         failed\u001b[38;5;241m.\u001b[39mappend(msg\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 2118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForImageClassification requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # Updated paths for local execution\n",
    "    train_annotations_path = \"archive/annots_arrs/annot_arrs_train.csv\"\n",
    "    val_annotations_path = \"archive/annots_arrs/annot_arrs_val.csv\"\n",
    "    img_dir = \"archive/img_arrs/\"\n",
    "    model_path = \"best_vit_emotic.pth\"\n",
    "    \n",
    "    batch_size = 8  # Reduced batch size for Mac compatibility\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    num_classes = 26\n",
    "    \n",
    "    # Use slower image processor to avoid PyTorch version conflicts\n",
    "    feature_extractor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=False)\n",
    "    \n",
    "    print(\"Parsing annotations...\")\n",
    "    train_annotations, class_counts = parse_annotations(train_annotations_path)\n",
    "    val_annotations, _ = parse_annotations(val_annotations_path)\n",
    "    \n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = EMOTICDataset(train_annotations, img_dir, feature_extractor, num_categories=num_classes)\n",
    "    val_dataset = EMOTICDataset(val_annotations, img_dir, feature_extractor, num_categories=num_classes)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    \n",
    "    # Alternative loading method to bypass AutoModel issues\n",
    "    try:\n",
    "        # First attempt: Direct ViT import (bypasses Auto class checks)\n",
    "        from transformers import ViTForImageClassification\n",
    "        \n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            \"google/vit-base-patch16-224\",\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        print(\"Model loaded successfully using direct ViT import!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Direct import failed: {e}\")\n",
    "        print(\"Trying alternative method...\")\n",
    "        \n",
    "        try:\n",
    "            # Second attempt: Import from models module\n",
    "            from transformers.models.vit import ViTForImageClassification\n",
    "            \n",
    "            model = ViTForImageClassification.from_pretrained(\n",
    "                \"google/vit-base-patch16-224\",\n",
    "                num_labels=num_classes,\n",
    "                ignore_mismatched_sizes=True\n",
    "            ).to(device)\n",
    "            print(\"Model loaded successfully using models module!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Alternative import also failed: {e}\")\n",
    "            print(\"Falling back to original AutoModel (may fail)...\")\n",
    "            \n",
    "            # Fallback to original method\n",
    "            model = AutoModelForImageClassification.from_pretrained(\n",
    "                \"google/vit-base-patch16-224\",\n",
    "                num_labels=num_classes,\n",
    "                ignore_mismatched_sizes=True\n",
    "            ).to(device)\n",
    "    \n",
    "    # Replace classifier\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(model.config.hidden_size, num_classes)\n",
    "    ).to(device)\n",
    "    \n",
    "    # Freeze ViT parameters, only train classifier\n",
    "    for param in model.vit.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    train_model_with_dynamic_weights(\n",
    "        model, train_loader, val_loader, optimizer, num_epochs, device, model_path, class_counts\n",
    "    )\n",
    "    \n",
    "    print(\"Calculating dynamic thresholds...\")\n",
    "    thresholds = calculate_dynamic_thresholds(model, val_loader, device)\n",
    "    \n",
    "    print(\"Final validation with dynamic thresholds...\")\n",
    "    final_map, final_f1, final_precision, final_recall = validate_model(model, val_loader, device, thresholds=thresholds)\n",
    "    print(f\"Final Results - mAP: {final_map:.4f}, F1: {final_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
