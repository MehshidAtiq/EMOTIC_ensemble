{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e99013d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.2 in ./.venv/lib/python3.9/site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision==0.16.2 in ./.venv/lib/python3.9/site-packages (0.16.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch==2.1.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch==2.1.2) (4.14.1)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch==2.1.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch==2.1.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch==2.1.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch==2.1.2) (2024.6.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from torchvision==0.16.2) (1.26.4)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from torchvision==0.16.2) (2.32.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.9/site-packages (from torchvision==0.16.2) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch==2.1.2) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->torchvision==0.16.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->torchvision==0.16.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->torchvision==0.16.2) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->torchvision==0.16.2) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "Requirement already satisfied: transformers==4.36.2 in ./.venv/lib/python3.9/site-packages (4.36.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (1.3.0)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.9/site-packages (10.0.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (4.66.4)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (3.7.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers==4.36.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.venv/lib/python3.9/site-packages (from transformers==4.36.2) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers==4.36.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers==4.36.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers==4.36.2) (2025.7.34)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers==4.36.2) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.9/site-packages (from transformers==4.36.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.9/site-packages (from transformers==4.36.2) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (1.1.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->transformers==4.36.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers==4.36.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->transformers==4.36.2) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->transformers==4.36.2) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Optional installs (uncomment if needed)\n",
    "# -------------------------\n",
    "!pip install torch==2.1.2 torchvision==0.16.2\n",
    "!pip install transformers==4.36.2 pandas numpy scikit-learn pillow tqdm seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d47789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading test annotations...\n",
      "Building ViT and loading weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([26]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([26, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from runs/vit_emotic_20250819-192210/best_vit_emotic.pth\n",
      "Loaded thresholds from runs/vit_emotic_20250819-192210/thresholds.npy with shape (26,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing ViT: 100%|██████████| 228/228 [03:16<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing 0 NaN values with 0...\n",
      "Total predictions shape: (7279, 26)\n",
      "Contains NaN: False\n",
      "Total NaN values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 339\u001b[0m\n\u001b[1;32m    336\u001b[0m         thresholds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull((NUM_CLASSES,), \u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m vit_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 244\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, loader, thresholds)\u001b[0m\n\u001b[1;32m    242\u001b[0m ap \u001b[38;5;241m=\u001b[39m calculate_average_precision(all_t, all_s)\n\u001b[1;32m    243\u001b[0m mAP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(ap[ap \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(ap \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 244\u001b[0m c_f1_per \u001b[38;5;241m=\u001b[39m calculate_label_based_f1(all_t, \u001b[43mpreds\u001b[49m)\n\u001b[1;32m    245\u001b[0m c_f1_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(c_f1_per[c_f1_per \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(c_f1_per \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    246\u001b[0m o_f1 \u001b[38;5;241m=\u001b[39m calculate_example_based_f1(all_t, preds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ViT EMOTIC Testing and Evaluation\n",
    "\n",
    "Directory layout expected (from your training notebook):\n",
    "- archive/annots_arrs/annot_arrs_train.csv\n",
    "- archive/annots_arrs/annot_arrs_val.csv\n",
    "- archive/annots_arrs/annot_arrs_test.csv\n",
    "- archive/img_arrs/*.npy                      # RGB or grayscale arrays\n",
    "Outputs from training used here:\n",
    "- best_vit_emotic.pth                         # model weights\n",
    "\n",
    "Optional:\n",
    "- vit_dynamic_thresholds.npy                  # dynamic thresholds; if absent we compute from val, else 0.5\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------\n",
    "# Optional installs (uncomment if needed)\n",
    "# -------------------------\n",
    "# !pip install torch==2.1.2 torchvision==0.16.2\n",
    "# !pip install transformers==4.36.2 pandas numpy scikit-learn pillow tqdm seaborn matplotlib\n",
    "\n",
    "# -------------------------\n",
    "# Imports & setup\n",
    "# -------------------------\n",
    "import os, warnings, numpy as np, pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# -------------------------\n",
    "# Device (MPS → CUDA → CPU)\n",
    "# -------------------------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------\n",
    "# Constants\n",
    "# -------------------------\n",
    "EMOTION_CATEGORIES = [\n",
    "    'Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
    "    'Confidence', 'Disapproval', 'Disconnection', 'Disquietment', 'Doubt',\n",
    "    'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue',\n",
    "    'Fear', 'Happiness', 'Pain', 'Peace', 'Pleasure',\n",
    "    'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy',\n",
    "    'Yearning'\n",
    "]\n",
    "NUM_CLASSES = 26\n",
    "\n",
    "# -------------------------\n",
    "# Annotations (matches your ViT notebook)\n",
    "#   - Uses CSV column slice df.columns[8:34] as the 26 label columns\n",
    "#   - Uses 'Crop_name' for the .npy file name\n",
    "# -------------------------\n",
    "def parse_annotations(csv_path, num_categories=NUM_CLASSES):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    category_columns = df.columns[8:8+num_categories]  # 8..33 inclusive → 26 cols\n",
    "\n",
    "    annotations = []\n",
    "    for _, row in df.iterrows():\n",
    "        cats = [int(i) for i, v in enumerate(row[category_columns]) if v == 1]\n",
    "        annotations.append({\"filename\": row[\"Crop_name\"], \"categories\": cats})\n",
    "    return annotations\n",
    "\n",
    "# -------------------------\n",
    "# Dataset for ViT\n",
    "#   - Loads .npy → PIL → AutoImageProcessor\n",
    "#   - Returns (pixel_values_tensor, labels_tensor)\n",
    "# -------------------------\n",
    "class EMOTICViTDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, image_processor: AutoImageProcessor, num_categories=NUM_CLASSES):\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = image_processor\n",
    "        self.num_categories = num_categories\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.annotations[idx]\n",
    "        img_path = os.path.join(self.img_dir, entry['filename'])\n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Missing image: {img_path}\")\n",
    "\n",
    "        arr = np.load(img_path)\n",
    "        # Ensure RGB\n",
    "        if arr.ndim == 2:\n",
    "            arr = np.stack([arr]*3, axis=-1)\n",
    "        elif arr.shape[-1] != 3:\n",
    "            raise ValueError(f\"Unexpected image shape {arr.shape} for {img_path}\")\n",
    "\n",
    "        # Convert to PIL (uint8 if needed)\n",
    "        if arr.dtype != np.uint8:\n",
    "            arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "        pil = Image.fromarray(arr)\n",
    "\n",
    "        inputs = self.processor(images=pil, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # (3,224,224)\n",
    "\n",
    "        labels = torch.zeros(self.num_categories, dtype=torch.float32)\n",
    "        for c in entry['categories']:\n",
    "            if 0 <= c < self.num_categories:\n",
    "                labels[c] = 1.0\n",
    "\n",
    "        return pixel_values, labels\n",
    "\n",
    "# -------------------------\n",
    "# Build ViT model same way as training\n",
    "# - Base: google/vit-base-patch16-224\n",
    "# - Multi-label, custom head: Dropout(0.3) + Linear(hidden_size→26)\n",
    "# -------------------------\n",
    "def build_vit_model(num_classes=NUM_CLASSES):\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        \"google/vit-base-patch16-224\",\n",
    "        num_labels=num_classes,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(model.config.hidden_size, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Metrics\n",
    "# -------------------------\n",
    "def calculate_average_precision(y_true, y_scores):\n",
    "    ap = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        if y_true[:, i].sum() > 0:\n",
    "            ap.append(average_precision_score(y_true[:, i], y_scores[:, i]))\n",
    "        else:\n",
    "            ap.append(0.0)\n",
    "    return np.array(ap)\n",
    "\n",
    "def calculate_label_based_f1(y_true, y_pred):\n",
    "    f1s = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        if y_true[:, i].sum() > 0 or y_pred[:, i].sum() > 0:\n",
    "            f1s.append(f1_score(y_true[:, i], y_pred[:, i], average=\"binary\", zero_division=0))\n",
    "        else:\n",
    "            f1s.append(0.0)\n",
    "    return np.array(f1s)\n",
    "\n",
    "def calculate_example_based_f1(y_true, y_pred):\n",
    "    f1s = []\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i].sum() > 0 or y_pred[i].sum() > 0:\n",
    "            f1s.append(f1_score(y_true[i], y_pred[i], average=\"binary\", zero_division=0))\n",
    "        else:\n",
    "            f1s.append(0.0)\n",
    "    return float(np.mean(f1s))\n",
    "\n",
    "# -------------------------\n",
    "# Thresholding\n",
    "# - If vit_dynamic_thresholds.npy exists → load\n",
    "# - Else if val CSV provided → compute per-class best F1 thresholds on val\n",
    "# - Else → use 0.5\n",
    "# -------------------------\n",
    "def compute_dynamic_thresholds(model, loader):\n",
    "    model.eval()\n",
    "    all_t, all_p = [], []\n",
    "    with torch.no_grad():\n",
    "        for px, labels in loader:\n",
    "            px = px.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(px).logits\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_t.append(labels.cpu().numpy())\n",
    "            all_p.append(probs)\n",
    "    all_t = np.vstack(all_t)\n",
    "    all_p = np.vstack(all_p)\n",
    "\n",
    "    thresholds = []\n",
    "    grid = np.linspace(0.05, 0.95, 19)\n",
    "    for i in range(all_p.shape[1]):\n",
    "        best_f1, best_th = -1.0, 0.5\n",
    "        for th in grid:\n",
    "            preds = (all_p[:, i] > th).astype(int)\n",
    "            _, _, f1, _ = precision_recall_fscore_support(all_t[:, i], preds, average=\"binary\", zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_th = f1, th\n",
    "        thresholds.append(best_th)\n",
    "    return np.array(thresholds, dtype=np.float32)\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation loop\n",
    "# -------------------------\n",
    "def evaluate_model(model, loader, thresholds):\n",
    "    model.eval()\n",
    "    all_t, all_s = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for px, labels in tqdm(loader, desc=\"Testing ViT\"):\n",
    "            px = px.to(device)\n",
    "            logits = model(px).logits\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            \n",
    "            # Fix Aversion class (index 4) NaN values\n",
    "            if np.isnan(probs[:, 4]).any():\n",
    "                probs[:, 4] = 0.0  # or 0.5 for neutral probability\n",
    "            \n",
    "            all_t.append(labels.numpy())\n",
    "            all_s.append(probs)\n",
    "    \n",
    "    all_t = np.vstack(all_t)\n",
    "    all_s = np.vstack(all_s)\n",
    "    \n",
    "    # Fix NaN values before calculating metrics\n",
    "    print(f\"Replacing {np.isnan(all_s).sum()} NaN values with 0...\")\n",
    "    all_s = np.nan_to_num(all_s, nan=0.0)\n",
    "    \n",
    "    # Report NaN statistics\n",
    "    print(f\"Total predictions shape: {all_s.shape}\")\n",
    "    print(f\"Contains NaN: {np.isnan(all_s).any()}\")\n",
    "    print(f\"Total NaN values: {np.isnan(all_s).sum()}\")\n",
    "    \n",
    "    # Continue with metrics...\n",
    "\n",
    "    # metrics\n",
    "    ap = calculate_average_precision(all_t, all_s)\n",
    "    mAP = float(np.mean(ap[ap > 0])) if np.any(ap > 0) else 0.0\n",
    "    c_f1_per = calculate_label_based_f1(all_t, preds)\n",
    "    c_f1_mean = float(np.mean(c_f1_per[c_f1_per > 0])) if np.any(c_f1_per > 0) else 0.0\n",
    "    o_f1 = calculate_example_based_f1(all_t, preds)\n",
    "\n",
    "    return {\n",
    "        \"ap_per_category\": ap,\n",
    "        \"mAP\": mAP,\n",
    "        \"c_f1_per_category\": c_f1_per,\n",
    "        \"c_f1_mean\": c_f1_mean,\n",
    "        \"o_f1\": o_f1,\n",
    "        \"predictions\": preds,\n",
    "        \"targets\": all_t,\n",
    "        \"scores\": all_s,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Visualization\n",
    "# -------------------------\n",
    "def plot_f1_scores(results, emotion_categories):\n",
    "    # single-model bar + side-by-side self-compare (same data duplicated for layout symmetry)\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    x = np.arange(len(emotion_categories))\n",
    "    bars = ax.bar(x, results['c_f1_per_category'], alpha=0.85)\n",
    "    ax.set_xlabel('Emotion Categories'); ax.set_ylabel('F1 Score'); ax.set_ylim([0,1])\n",
    "    ax.set_title(f'ViT Model - F1 per Category (Mean C-F1: {results[\"c_f1_mean\"]:.3f})')\n",
    "    ax.set_xticks(x); ax.set_xticklabels(emotion_categories, rotation=45, ha='right')\n",
    "    for b in bars:\n",
    "        h = b.get_height()\n",
    "        if h > 0:\n",
    "            ax.text(b.get_x() + b.get_width()/2., h, f'{h:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    plt.tight_layout(); plt.savefig('vit_f1_scores_comparison.png', dpi=300, bbox_inches='tight'); plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, results['c_f1_per_category'], width, label='ViT', alpha=0.85)\n",
    "    ax.bar(x + width/2, results['c_f1_per_category'], width, label='ViT (dup)', alpha=0.35)\n",
    "    ax.legend(); ax.set_ylim([0,1]); ax.set_xlabel('Emotion Categories'); ax.set_ylabel('F1 Score')\n",
    "    ax.set_title('F1 Score (ViT)'); ax.set_xticks(x); ax.set_xticklabels(emotion_categories, rotation=45, ha='right')\n",
    "    plt.tight_layout(); plt.savefig('vit_f1_scores_direct_comparison.png', dpi=300, bbox_inches='tight'); plt.show()\n",
    "\n",
    "def plot_heatmap(results, emotion_categories):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    df = pd.DataFrame({'ViT': results['c_f1_per_category']}, index=emotion_categories)\n",
    "    sns.heatmap(df, annot=True, fmt='.2f', cmap='YlOrRd', vmin=0, vmax=1, cbar_kws={'label':'F1'})\n",
    "    ax.set_title('ViT F1 Heatmap by Emotion'); ax.set_xlabel('Model'); ax.set_ylabel('Emotion')\n",
    "    plt.tight_layout(); plt.savefig('vit_f1_scores_heatmap.png', dpi=300, bbox_inches='tight'); plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths (match your training notebook layout)\n",
    "    test_annotations_path = \"archive/annots_arrs/annot_arrs_test.csv\"\n",
    "    # val_annotations_path  = \"archive/annots_arrs/annot_arrs_val.csv\"  # used only if thresholds file is missing\n",
    "    img_dir = \"archive/img_arrs\"\n",
    "    vit_model_path = \"runs/vit_emotic_20250819-192210/best_vit_emotic.pth\"\n",
    "    thresholds_path = \"runs/vit_emotic_20250819-192210/thresholds.npy\"  \n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    # Processor\n",
    "    feature_extractor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n",
    "\n",
    "    # Data: test\n",
    "    print(\"Loading test annotations...\")\n",
    "    test_ann = parse_annotations(test_annotations_path, num_categories=NUM_CLASSES)\n",
    "    test_ds = EMOTICViTDataset(test_ann, img_dir=img_dir, image_processor=feature_extractor, num_categories=NUM_CLASSES)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Build/load model\n",
    "    print(\"Building ViT and loading weights...\")\n",
    "    model = build_vit_model(num_classes=NUM_CLASSES).to(device)\n",
    "    state = torch.load(vit_model_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    print(\"Loaded weights from\", vit_model_path)\n",
    "\n",
    "    # Thresholds\n",
    "    if os.path.exists(thresholds_path):\n",
    "        thresholds = np.load(thresholds_path)\n",
    "        print(f\"Loaded thresholds from {thresholds_path} with shape {thresholds.shape}\")\n",
    "    else:\n",
    "        if os.path.exists(val_annotations_path):\n",
    "            print(\"Threshold file not found. Computing dynamic thresholds on validation set...\")\n",
    "            val_ann = parse_annotations(val_annotations_path, num_categories=NUM_CLASSES)\n",
    "            val_ds = EMOTICViTDataset(val_ann, img_dir=img_dir, image_processor=feature_extractor, num_categories=NUM_CLASSES)\n",
    "            val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "            thresholds = compute_dynamic_thresholds(model, val_loader)\n",
    "            np.save(thresholds_path, thresholds)\n",
    "            print(f\"Saved thresholds to {thresholds_path}\")\n",
    "        else:\n",
    "            print(\"No thresholds file or val CSV—using 0.5 for all classes.\")\n",
    "            thresholds = np.full((NUM_CLASSES,), 0.5, dtype=np.float32)\n",
    "\n",
    "    # Evaluate\n",
    "    vit_results = evaluate_model(model, test_loader, thresholds)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VIT EVALUATION RESULTS (TEST)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Mean Average Precision (mAP): {vit_results['mAP']:.4f}\")\n",
    "    print(f\"C-F1 (Label-based F1):       {vit_results['c_f1_mean']:.4f}\")\n",
    "    print(f\"O-F1 (Example-based F1):     {vit_results['o_f1']:.4f}\")\n",
    "\n",
    "    # Per-category table\n",
    "    results_df = pd.DataFrame({\n",
    "        'Emotion': EMOTION_CATEGORIES,\n",
    "        'ViT_AP': vit_results['ap_per_category'],\n",
    "        'ViT_F1': vit_results['c_f1_per_category'],\n",
    "    }).round(4)\n",
    "\n",
    "    print(\"\\nDetailed Per-Category Metrics:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    results_df.to_csv('per_category_results_vit.csv', index=False)\n",
    "    print(\"\\nSaved: per_category_results_vit.csv\")\n",
    "\n",
    "    # Visuals\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    plot_f1_scores(vit_results, EMOTION_CATEGORIES)\n",
    "    plot_heatmap(vit_results, EMOTION_CATEGORIES)\n",
    "\n",
    "    # Quick “best/worst” change within ViT itself (not vs another model)\n",
    "    top5 = results_df.nlargest(5, 'ViT_F1')[['Emotion', 'ViT_F1']]\n",
    "    bottom5 = results_df.nsmallest(5, 'ViT_F1')[['Emotion', 'ViT_F1']]\n",
    "    print(\"\\nTop 5 ViT categories by F1:\")\n",
    "    for _, r in top5.iterrows():\n",
    "        print(f\"  {r['Emotion']}: {r['ViT_F1']:.4f}\")\n",
    "    print(\"\\nBottom 5 ViT categories by F1:\")\n",
    "    for _, r in bottom5.iterrows():\n",
    "        print(f\"  {r['Emotion']}: {r['ViT_F1']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
